[[integration]]
== Integrating with OpenShift
include::includes/header.adoc[]

While it is simple to <<deployment,deploy an image on OpenShift>>,
there are certain guidelines and integration techniques that should be
adhered to.

[[creating-images]]
=== Creating Images

The OpenShift documentation contains a section on
https://docs.openshift.com/container-platform/latest/creating_images/guidelines.html[image creation guidelines],
so rather than repeat that information here, it's recommended to be familiar
with the practices in that document. Points of particular interest will be
included in the rest of this guide.

[[recommended-practices]]
=== Recommended Practices

There are some recommended practices for your images to ensure full compatibility with Red Hat Products. 

* Containers should not run as root.
** Containers running as root represent a security vulnerability as a process that breaks out of the container will retain root privileges on the host system.
** Containers run as root by default - More information on users can be found <<users,here>>.
* Containers should not request host-level privileges.
** Containers requiring host-level privileges may not function correctly in all environments - namely those that the application deployer does not have full control over the host system.
** OpenShift Online and OpenShift Dedicated do not support privileged containers.
* Containers should use a Red Hat provided base image. They should also not modify content provided by Red Hat packages or layers.
** The following dockerfile snippet shows a way to to build an image on rhel.
+
----
FROM registry.access.redhat.com/rhel7
----
** This ensures that the application's runtime dependencies are fully supported by Red Hat.

[[env-var]]
=== Environment Variables

It is impractical to rebuild an image for each possible configuration,
especially if the image is owned by someone else. The recommended mechanism
for configuring a service running in a container is through environment
variables.

Environment variables are set in the deployment configuration. They can be
set when the configuration is first created or can be added/modified/removed
after an application has been deployed. It is important to realize that
changes to the environment variables constitute a deployment configuration
change. The existing pods are not modified; rather, new pods are deployed with
the changed configuration and will automatically replace the existing ones
(this is easily seen on the UI overview page with the graphical depictions of
scaling up and down).

This paradigm is used both for images created for deployment as well as those
intended to be used as builder images with
<<deployment.adoc#source-to-image, source to image>>. For example,
the Python builder image will, by default, attempt to run a file named
`app.py`. If an application cannot be modified to use this naming scheme,
the environment variable `APP_FILE` can be specified to indicate a new
script to run when the container is launched.

==== Example

Below is the output for the deployment configuration of a simple web
application. The application is written such that it will output "Hello
World" when it is not overridden with environment variables.

As a reminder, the list of a particular resource can be viewed using the
`get` command, followed by the resource type in question (`dc` is an
abbreviaton for deployment configuration):

[source,bash,options="nowrap"]
----
$ oc get dc
NAME         REVISION   DESIRED   CURRENT   TRIGGERED BY
python-web   3          1         1         config,image(python-web:latest)
----

The details of the configuration can be displayed with the `describe`
command (some of the irrelevant information has been removed for brevity):

[source,bash,options="nowrap"]
----
$ oc describe dc python-web
Name:        python-web
Namespace:   python-web
Created:     10 days ago
Labels:      app=python-web
Annotations: openshift.io/generated-by=OpenShiftNewApp
Latest Version:   3
Selector:    app=python-web,deploymentconfig=python-web
Replicas:    1
Triggers:    Config, Image(python-web@latest, auto=true)
Strategy:    Rolling
Template:
    Labels:    app=python-web
    deploymentconfig=python-web
    Annotations:   openshift.io/container.python-web.image.entrypoint=["/bin/sh","-c","cd /src/www; /bin/bash -c 'python3 -u /src/web.py'"]
    openshift.io/generated-by=OpenShiftNewApp
    Containers:
    python-web:
        Image:                 jdob/python-web@sha256:3f87be1825405ee8c7da23d7a6916090ecbb2d6e7b04fcd0fd1dc194173d2bc0
        Port:                  8080/TCP
        Volume Mounts:         <none>
        Environment Variables: <none>
    No volumes.
----

Note that there are no environment variables set for the application. Viewing
the application (through its route), displays the default "Hello World" text:

[source,bash,options="nowrap"]
----
$ oc get route
NAME         HOST/PORT                                    PATH      SERVICES     PORT       TERMINATION
python-web   python-web-python-web.apps.10.2.2.2.xip.io             python-web   8080-tcp

$ curl http://python-web-python-web.apps.10.2.2.2.xip.io
Hello World
----

There are a few options for editing environment variables. The UI can be
used to navigate to the deployment configuration. The "Environment" tab
can be used to view and modify environment variables for the configuration.
When changes are saved by pressing the "Save" button, a new deployment
is triggered using the new configuration values.

image::images/env_variables.png[]

Alternatively, the CLI's `edit` command can be used to interactively edit
the YAML representation of many resources. This command, called by specifying
a resource type and name, opens a text editor in which changes can be made.
When the file is saved and the editor is closed, the changes are sent to
the server and the appropriate action is taken. In this case, the change in
configuration will cause a redeployment.

Below is a snippet of the deployment configuration while being edited
(removed sections are replaced with `[snip]` for readability). The
changes made are highlighted:

[source,bash,options="nowrap"]
----
$ oc edit dc python-web
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: DeploymentConfig
metadata:
    [snip]
spec:
    [snip]
    template:
    metadata:
        [snip]
    spec:
        containers:
        - env:
        - name: TEXT <1>
          value: Goodbye World <1>
        image: jdob/python-web@sha256:3f87be1825405ee8c7da23d7a6916090ecbb2d6e7b04fcd0fd1dc194173d2bc0
        imagePullPolicy: Always
        name: python-web
----
<1> A new environment variable named `TEXT` is introduced into the container.

If present, the value of the `TEXT` variable is output by the web server when
it is accessed. For reference, the relevant Python line in the application is:

[source,python,options="nowrap"]
----
m = os.environ.get('TEXT', None) or 'Hello World'
----

At this point, there are a few ways to monitor the changes being made. The
UI presents a graphical view of the existing pods scaling down while new ones
are created with the new configuration. The CLI's `status` command can be
used to show that a new deployment was made:

[source,bash,options="nowrap"]
----
$ oc status                                                                                                                                                                        1 â†µ
In project python-web on server https://localhost:8443

http://python-web-python-web.apps.10.2.2.2.xip.io to pod port 8080-tcp (svc/python-web)
    dc/python-web deploys istag/python-web:latest
    deployment #2 deployed 9 minutes ago - 1 pod
    deployment #1 deployed 48 minutes ago
----

Notice that a new deployment was made, corresponding to the updated deployment
configuration that was submitted. As proof of the environment variable's
presence in the container, the previous `curl` command can be run again:

[source,bash,options="nowrap"]
----
$ curl http://python-web-python-web.apps.10.2.2.2.xip.io
Goodbye World
----

[[users]]
=== Users

One of the more common obstacles encountered with creating new images
revolves around the user running the container process. By default, Docker
containers are run as root. This can become a
http://blog.dscpl.com.au/2015/12/don-run-as-root-inside-of-docker.html[security issue]
as any process that breaks out of the container will retain the same privileges
on the host machine; root in a container would have access to root on the host.

By default, OpenShift will issue a warning when loading an image defined to
run as root and, in many cases, the deployment will fail with some form of
permission error. These failures are due to the fact that OpenShift creates a
random, non-privileged user (with no corresponding UID on the host machine)
and runs the container with that user. This is an added security benefit
provided by OpenShift and, while not difficult, must be acknowledged when
creating images.

Since OpenShift is generating a random UID, the solution isn't as simple as
http://blog.dscpl.com.au/2015/12/overriding-user-docker-containers-run-as.html[creating and using a user]
(by its name) within the container. There are potential security issues
where a created user can still give itself root privileges. The use of a
random ID, specified by OpenShift, also supports added security for
multi-tenancy by forcing persistent storage volume UIDs to be unique for each
project.

In short, since OpenShift runs containers as a randomized, non-privileged user,
the image must be constructed with those limitations in mind.

The common solution is to make the necessary files and directories
http://blog.dscpl.com.au/2015/12/random-user-ids-when-running-docker.html[writable by the root group].

==== Example

Below is a snippet from a Dockerfile used to run httpd as a non-privileged
container. This setup will host pages from the `/opt/app-root` directory.
For brevity, the Dockerfile `EXPOSE` and corresponding
httpd configuration changes to serve on a non-privileged port are not
included in the snippet.

[source,bash,options="nowrap"]
----
# Create a non root account called 'default' to be the owner of all the
# files which the Apache httpd server will be hosting. This account
# needs to be in group 'root' (gid=0) as that is the group that the
# Apache httpd server would use if the container is later run with a
# unique user ID not present in the host account database, using the
# command 'docker run -u'.

ENV HOME=/opt/app-root

RUN mkdir -p ${HOME} && \
    useradd -u 1001 -r -g 0 -d ${HOME} -s /sbin/nologin \ <1>
            -c "Default Application User" default

# Fixup all the directories under the account so they are group writable
# to the 'root' group (gid=0) so they can be updated if necessary, such
# as would occur if using 'oc rsync' to copy files into a container.

RUN chown -R 1001:0 /opt/app-root && \
    find ${HOME} -type d -exec chmod g+ws {} \; <2>

# Ensure container runs as non root account from its home directory.
WORKDIR ${HOME}
USER 1001 <3>
----
<1> The user is created through a numeric ID.
<2> Group permissions are given to the necessary directories.
<3> Indicate that the image should be run as the non-root user.

Note the usage of a numeric UID instead of the named user. This is done for
portability across hosting providers and will pass checks to ensure that,
at very least, the container is not being run as root (this check is
impossible using named users).

[[labels]]
=== Labels

In this context, labels refer to the Docker concept of labels: metadata
on an image. These are specified in the Dockerfile and are included in the
built image.

Partner container certification requires that images include the following
labels:

* name
* vendor
* version
* release

The container partner team provides a sample Dockerfile that can be used
as a template for images suitable for certification. It can be found
on
https://github.com/RHsyseng/container-rhel-examples/blob/master/starter/Dockerfile[GitHub].

==== Example

The following snippet uses the Dockerfile `LABEL` directive to define
the minimum required labels:

[source,Dockerfile,options="nowrap"]
----
LABEL name="jdob/python-web" \
      vendor="Red Hat" \
      version="1.0" \
      release="1"
----

The labels can be viewed using the `docker inspect` command (the output
below is truncated:

[source,bash,options="nowrap"]
----
$ docker inspect --format {{.ContainerConfig.Labels}} jdob/python-web
map[name:jdob/python-web release:1 vendor:Red Hat version:1.0]
----

[[api-auth]]
=== Authenticating to the OpenShift APIs

Service accounts may be used to authenticate against the OpenShift API without
the need to use a regular user's credentials. This can be used for
integrations that require extra information about the running system in which
they are deployed, such as for discovery or monitoring purposes. Service
accounts are identified by a username and its roles can be manipulated
in the same way.

In order to properly configure permissions for a service account, some
understanding of the security role system is required.

[[scc]]
=== Security Context Constraints

[WARNING]
====
Operations on security context constraints can only be performed
by an admin user, including listing or describing existing SCCs.
====

Security Context Constraints (SCC for short) define a set of access
permissions. Users and service accounts are added to SCCs to permit them
the privileges defined by the SCC.

A list of all defined SCCs can be retrieved using the `get` command and
the `scc` resource type:

[source,bash,options="nowrap"]
----
NAME               PRIV      CAPS      SELINUX     RUNASUSER          FSGROUP     SUPGROUP    PRIORITY   READONLYROOTFS   VOLUMES
anyuid             false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    10         false            [configMap downwardAPI emptyDir persistentVolumeClaim secret]
hostaccess         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath persistentVolumeClaim secret]
hostmount-anyuid   false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim secret]
hostnetwork        false     []        MustRunAs   MustRunAsRange     MustRunAs   MustRunAs   <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim secret]
nonroot            false     []        MustRunAs   MustRunAsNonRoot   RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim secret]
privileged         true      []        RunAsAny    RunAsAny           RunAsAny    RunAsAny    <none>     false            [*]
restricted         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim secret]
----

Specific details are displayed using the `describe` command. Below is the
output for the default `restricted` SCC:

[source,bash,options="nowrap"]
----
Name:                        restricted
Priority:                    <none>
Access:
    Users:                     <none>
    Groups:                    system:authenticated
Settings:
    Allow Privileged:              false
    Default Add Capabilities:      <none>
    Required Drop Capabilities:    KILL,MKNOD,SYS_CHROOT,SETUID,SETGID
    Allowed Capabilities:          <none>
    Allowed Volume Types:          configMap,downwardAPI,emptyDir,persistentVolumeClaim,secret
    Allow Host Network:            false
    Allow Host Ports:              false
    Allow Host PID:                false
    Allow Host IPC:                false
    Read Only Root Filesystem:     false
    Run As User Strategy: MustRunAsRange
    UID:                     <none>
    UID Range Min:           <none>
    UID Range Max:           <none>
    SELinux Context Strategy: MustRunAs
    User:                    <none>
    Role:                    <none>
    Type:                    <none>
    Level:                   <none>
    FSGroup Strategy: MustRunAs
    Ranges:                  <none>
    Supplemental Groups Strategy: RunAsAny
    Ranges:                  <none>
----

The SCC description includes information on what is permitted to users in the
SCC. The `Access` section indicates which users are granted access to the
SCC. Note that service accounts are treated as users in this context and
will appear in this list as well.

Users are granted access to an SCC through the admin policy (`adm policy`)
command:

[source,bash,options="nowrap"]
----
$ oc adm policy add-scc-to-user restricted onboard

$ oc describe scc restricted
Name:                    restricted
Priority:                <none>
Access:
    Users:                 onboard
    Groups:                system:authenticated
[output truncated]
----

[[service-accounts]]
=== Service Accounts

Service accounts exist within the scope of a particular project. Given that,
cluster admin privileges are not required. Like other API objects, they are
created and deleted through the `create` command:

[source,bash,options="nowrap"]
----
$ oc create serviceaccount onboard-sa
serviceaccount "onboard-sa" created

$ oc get sa
NAME         SECRETS   AGE
builder      2         <invalid>
default      2         <invalid>
deployer     2         <invalid>
onboard-sa   2         <invalid>
----

In the example above, the service account will be created in the currently
active project. A different project may be specified using the `-n` flag.

All projects are configured with three default service accounts:

* *builder* - Build pods use this SCC to push images into the internal
  Docker registry and manipulate image streams.
* *deployer* - Used to view and edit replication controllers.
* *default* - Used to run all non-builder pods unless explicitly overridden.

Service accounts can be added to SCCs in the same way as users with one
notable exception. The username for the service account must be fully qualified
as a service account and identifying the project in which it exists. The
template for the user name is:

[source,bash,options="nowrap"]
----
system:serviceaccount:<project>:<sa-name>
----

For example, to add the previously created service account (assuming it was
under the project name `demo`):

[source,bash,options="nowrap"]
----
$ oc adm policy add-scc-to-user restricted system:serviceaccount:demo:onboard-sa

$ oc describe scc restricted
Name:                    restricted
Priority:                <none>
Access:
    Users:                 system:serviceaccount:demo:onboard-sa
    Groups:                system:authenticated
----

=== Authenticating as a Service Account

There are two ways to retrieve an API token for the service account.

==== Externally Retrieving a Token

The `describe` command can be used to show the tokens that were created
for a service account:

[source,bash,options="nowrap"]
----
$ oc describe sa onboard-sa                                                                                                                                                        1 â†µ
Name:		onboard-sa
Namespace:	guestbook
Labels:		<none>

Image pull secrets:	onboard-sa-dockercfg-myuk7

Mountable secrets: 	onboard-sa-token-tuwfj
                    onboard-sa-dockercfg-myuk7

Tokens:            	onboard-sa-token-n79y5
                    onboard-sa-token-tuwfj
----

In this case, the list `Tokens` is of interest. The token itself can be
retrieved through the `describe` command for the secret (the actual
token value is truncated for brevity):

[source,bash,options="nowrap"]
----
$ oc describe secret onboard-sa-token-n79y5                                                                                                                                        1 â†µ
Name:		onboard-sa-token-n79y5
Namespace:	guestbook
Labels:		<none>
Annotations:	kubernetes.io/service-account.name=onboard-sa
        kubernetes.io/service-account.uid=efe81599-bd6f-11e6-b14e-5254009f9a8b

Type:	kubernetes.io/service-account-token

Data
====
ca.crt:		1066 bytes
namespace:	9 bytes
token:		eyJhbGciOi...
----

Assuming the token value is saved to an environment variable named `TOKEN`,
the list of users can be retrieved with the following `curl` command:

[source,bash,options="nowrap"]
----
$ TOKEN="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"
$ curl -k "https://10.1.2.2:8443/oapi/v1/users/~" -H "Authorization: Bearer $TOKEN"                                                                                                 1 â†µ
    {
      "kind": "User",
      "apiVersion": "v1",
      "metadata": {
        "name": "system:serviceaccount:demo:onboard-sa",
        "selfLink": "/oapi/v1/users/system:serviceaccount:demo:onboard-sa",
        "creationTimestamp": null
      },
      "identities": null,
      "groups": [
        "system:serviceaccounts",
        "system:serviceaccounts:demo"
      ]
    }
----

==== From Within a Container

The API token for the service account associated with a deployment
configuration is automatically injected into each container when it is
created. The service account for a container can be changed from the default
to an account with the proper permissions based on the need. The token
is stored inside the container at:

[source,bash,options="nowrap"]
----
/var/run/secrets/kubernetes.io/serviceaccount/token
----

Using the same curl command as above:

[source,bash,options="nowrap"]
----
$ TOKEN="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"
$ curl -k "https://10.1.2.2:8443/oapi/v1/users/~" -H "Authorization: Bearer $TOKEN"                                                                                                 1 â†µ
    {
      "kind": "User",
      "apiVersion": "v1",
      "metadata": {
        "name": "system:serviceaccount:demo:default",
        "selfLink": "/oapi/v1/users/system:serviceaccount:demo:default",
        "creationTimestamp": null
      },
      "identities": null,
      "groups": [
        "system:serviceaccounts",
        "system:serviceaccounts:demo"
      ]
    }
----

This output was taken from a container with no additional configuration,
so the self reference refers to the project's `default` service account.

==== Example

An example PHP script using the container's token to access the API of the
OpenShift instance in which it is deployed can be
https://github.com/jdob-openshift/php-token-demo[found on GitHub].
It can be built and deployed as a
<<deployment#source-to-image,source to image>> application.

While it is not practical to repeat the code here, there are a few sections
of note.

[source,php,options="nowrap"]
----
$token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token";
$f = fopen($token_file, "r");
$token = fread($f, filesize($token_file));
fclose($f);
$auth = "Authorization: Bearer $token";
----

The block above reads in the contents of the token file to use for
authenticating against the API. The token is passed in the
`Authentication: Bearer` header.

[source,php,options="nowrap"]
----
$url = "https://openshift.default.svc.cluster.local/oapi/v1/users/~";
----

The alias `openshift.default.svc.cluster.local` is made available to all
containers by default and can be used to access the control plane for that
container.

[source,php,options="nowrap"]
----
curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, false);
----

The API is run on HTTPS. For simplicity and portability of this example,
verification of the SSL certificate is disabled. Extra steps may be
necessary to provide the proper CA for the container.

[[openshift-network-plugins]]
=== OpenShift Network plugins

For instances where the default network behaviour does not meet the user's needs, OpenShift supports the 
same plugin model as https://kubernetes.io/docs/concepts/cluster-administration/network-plugins[Kubernetes] 
does for networking pods.

==== Container Network Interfaces

CNIs are the recommended method for usage of network plugins. Any external networking solution can 
be used to plumb networking for OpenShift as long as it follows the CNI spec. Then, OpenShift 
needs to be launched with `networkPluginName: "cni"` in the master/node config yaml files.

When done through Ansible, provide `sdn_network_plugin_name=cni` as an option while installing 
OpenShift. The default behavior of the OpenShift Ansible installation allows a firewall passthrough for 
the VXLAN port (4789), so if a plugin needs other ports (for management/control/data) to be 
open, then the installer needs to be changed suitably.

More information about CNIs can be found in the http://kubernetes.io/docs/admin/network-plugins/#cni[Kubernetes documentation] and in the https://github.com/containernetworking/cni/blob/master/SPEC.md[CNI spec].

==== Requirements

OpenShift networking requires the following items to be kept in mind when writing CNI plugins in addition to the https://github.com/kubernetes/kubernetes/blob/release-1.3/docs/design/networking.md[basic Kubernetes requirements].

[red]#Red# items are not necessary for a functional OpenShift cluster, but some things will need to be worked around for the administrator's benefit.

. A plugin can follow the NetworkPolicy objects from Kubernetes and implement the user/admin 
intent on multi-tenancy. 
Alternatively, plugins can ignore multi-tenancy completely or implement a model where multi-tenancy is based on projects (i.e. Kubernetes namespaces), where:
** Each namespace should be treated like a tenant where its pods and services are isolated from another project's pods/services
** [red]#Support exists for operations like merge/join networks even when they belong to different namespaces#
. Certain services in the cluster will be run as infrastructure services (e.g. Load balancer, registry, DNS server(skydns)). 
The plugin should allow for a 'global' tenant which is-accessible-by/can-access all pods of the cluster. For example, a load balancer 
can run in two modes - private and global. The global load balancer should have access to all tenants/namespaces of the cluster. A 
private load balancer is one that is launched as a pod by a particular namespace, and this should obey tenant isolation rules.
. [red]#Access to all pods from the host - particularly important if kube-proxy is used by the SDN solution to support Kubernetes services. 
Please note that iptables based kube-proxy will be enabled by default in OpenShift. This will have to be overridden specially if the plugin 
wants a different behaviour.#
. Access to external networks by pods whether through NAT or direct access.
. OpenShift builds docker images as part of the developer workflow. The build is run through 'docker build' api. 
This means that docker's default networking will be invoked for this container (CNI/kube-plugin will not run as this is not a pod). 
These containers still need a network and access to external network (e.g. the internet e.g.).
. [red]#Respect the `PodSecurityContext::HostNetwork=true` for infra pods. Or provide an externally routable IP address to the pod. This is 
used for the load balancer pods which are the entry point for all external traffic funneling into the cluster.#
** Note that the HostPort<-->ContainerPort mapping will not be available by default if the CNI plugin is enabled (as the default docker networking is turned off). The plugin will have to implement this functionality by itself.